{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: The earnings date is taken as the ex-date\n",
    "\n",
    "1. Get list of company tickers from finviz\n",
    "2. Get earnings data report for each company in `companies`\n",
    "3. Make a DF row for each company row\n",
    "\n",
    "Columns to populate:\n",
    "For each company: For each earnings\n",
    "1. compute gap%\n",
    "2. find C1\n",
    "3. find O\n",
    "4. find market cap\n",
    "5. find distance from 52W high\n",
    "6. find recency of 52W high\n",
    "7. trading volume near announcement (at exdate)\n",
    "8. trading volume before announcement (at c1)\n",
    "\n",
    "FORMULAE:\n",
    "1. Market cap = Total shares outstanding * price at that date\n",
    "2. 52W_HIGH = \n",
    "3. 52W_RECENCY = \n",
    "4. \n",
    "\n",
    "\n",
    "CHANGES:\n",
    "1. Change 52week defintion to 252 days instead of 364 days.\n",
    "2. for the first week, do min (52 weeks, available weeks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stocks = pd.read_csv('../data/remaining.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_filters = [ 'Closed-End Fund - Foreign', 'Closed-End Fund - Equity', 'Asset Management', 'Closed-End Fund - Debt', 'Shell Companies', 'Exchange Traded Fund']\n",
    "filtered_stocks = stocks.loc[~stocks['Industry'].isin(ignore_filters)]\n",
    "filtered_stocks.to_csv('stocks/FINAL_US_STOCKS.csv', mode='w', sep=',')\n",
    "filtered_stocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_us_stocks = pd.read_csv('stocks/FINAL_US_STOCKS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_us_stocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET US STOCK LIST FROM FINVIZ\n",
    "\n",
    "from finvizfinance.screener.custom import Custom\n",
    "\n",
    "def get_us_stocks():\n",
    "    \"\"\"\n",
    "    Unused\n",
    "    \"\"\"\n",
    "    fcustom = Custom()\n",
    "    cols = [0, 1]\n",
    "    ignore_filters = [ 'Closed-End Fund - Foreign', 'Closed-End Fund - Equity', 'Closed-End Fund - Debt', 'Shell Companies', 'Exchange Traded Fund']\n",
    "    my_industries = list(set(get_fv_filter_options('Industry')) - set(ignore_filters))\n",
    "#     print(my_industries)\n",
    "    master_df = pd.DataFrame()\n",
    "    for industry in my_industries:\n",
    "        fcustom = Custom()\n",
    "        cols = [0,1]\n",
    "        my_filters = { 'Country': 'USA', 'Industry': industry}\n",
    "        fcustom.set_filter(filters_dict=my_filters)\n",
    "        df1 = fcustom.screener_view(columns=cols, sleep_sec=1, order='Ticker')\n",
    "        pd.concat([master_df, df1], ignore_index=True)\n",
    "        print('[+] Done {}'.format(industry))\n",
    "    return master_df\n",
    "#     print(\"stringified\", ','.join(my_industries))\n",
    "#     filters = { 'Country': 'USA', 'Industry': ','.join(my_industries)}\n",
    "#     fcustom.set_filter(filters_dict=filters)\n",
    "#     df1 = fcustom.screener_view(columns=cols, sleep_sec=1, order='Ticker')\n",
    "#     return df1\n",
    "\n",
    "def get_filters():\n",
    "    fcustom = Custom()\n",
    "    return fcustom.get_filters()\n",
    "\n",
    "def write_to_file(df, filename):\n",
    "    df.to_csv(rf\"{filename}\", header=None, sep=' ', mode='w')\n",
    "\n",
    "def get_fv_filter_options(filtername):\n",
    "    fcustom = Custom()\n",
    "    return fcustom.get_filter_options(filtername)\n",
    "\n",
    "write_to_file(get_us_stocks(), \"stocks/UPDATED_US_STOCKS.txt\")\n",
    "print(\"[+] Completed Task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACAKGE VERSION REQUIREMENTS - YAHOOQUERY\n",
    "import yahooquery as yq\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "print(yq.__version__) # >=2.3.0\n",
    "print(requests.__version__) # >= 2.28.2\n",
    "print(urllib3.__version__) # >= 1.26.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = yq.Ticker('META', asynchronous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = meta.history(period=\"max\", interval=\"1d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(fr\"tmp/META_ADJUSTED_OHLC.csv\", mode='w', sep=',', header=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OHLC DATA READING FROM US_STOCKS in stocks/\n",
    "# Note: When this was first run, we got all data (even ETFs, index funds, etc.)\n",
    "\n",
    "import time\n",
    "import os\n",
    "import yahooquery as yq\n",
    "import pandas as pd\n",
    "\n",
    "def read_dir(dirname):\n",
    "    lst = os.listdir(dirname)\n",
    "    lst = [e.split('_')[0] for e in lst]\n",
    "    return lst\n",
    "\n",
    "no_olhc = pd.read_csv('tmp/REST.csv')\n",
    "del no_olhc['Unnamed: 0']\n",
    "symbols = list(no_olhc['0'])\n",
    "\n",
    "def get_bulk_ohlc_data():\n",
    "  with open(\"logs/ohlc_errors.txt\", \"w\") as f:\n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "          company = yq.Ticker(symbol, asynchorous=True)\n",
    "          df = company.history(period=\"max\", interval=\"1d\")\n",
    "          df.to_csv(fr\"ohlc/{symbol}_ADJUSTED_OHLC.csv\", mode='w', sep=',', header=df.columns)\n",
    "          print(\"[+] Done \" + symbol)\n",
    "          time.sleep(0.5) # 2000 requests/hour\n",
    "        except Exception as e:\n",
    "          print(\"An exception occured: {}\".format(e))\n",
    "          f.write(str(e) + \"\\n\")\n",
    "          pass\n",
    "\n",
    "# get_bulk_ohlc_data()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# MAKE DIRECTORIES earnings/, logs/, income_stmts/ before running get_data()\n",
    "# This reads from FINAL_US_STOCKS.csv in stocks/\n",
    "# Make sure it exists at the desired path\n",
    "\n",
    "BASE_URL = 'https://alphavantage.co'\n",
    "AV_API_KEY = 'IHLNUQ0G66C0MJID'\n",
    "# ignore\n",
    "AV_API_KEY_2 = 'IINQBSLFU0AKNY53'\n",
    "AV_API_KEY_3 = 'L9X61VSB6U2BMPHW'\n",
    "AV_API_KEY_4 = 'FSMTFWDL6XFDQSLN'\n",
    "AV_API_KEY_5 = '7FMXJEH0JBVC8D7M'\n",
    "\n",
    "# keys = [\n",
    "#    AV_API_KEY,\n",
    "#    AV_API_KEY_2,\n",
    "#    AV_API_KEY_3,\n",
    "#    AV_API_KEY_4,\n",
    "#    AV_API_KEY_5\n",
    "# ]\n",
    "\n",
    "def get_bulk_earnings_data():\n",
    "   \"\"\"\n",
    "   Reads from earnings/ to get diff\n",
    "   Make sure FINAL_US_STOCKS.csv exists inside stocks/\n",
    "\n",
    "   \"\"\"\n",
    "   remaining =  list(set(read_symbols('stocks/FINAL_US_STOCKS.csv')) - set(parse_symbols('earnings/')))\n",
    "   print(\"Remaining\", len(remaining))\n",
    "   with open('logs/earnings_errors.txt', 'w') as f:\n",
    "    for symbol in remaining:\n",
    "        try:\n",
    "          df = av_get_earnings(AV_API_KEY, symbol)\n",
    "          if df is None:\n",
    "                raise KeyError()\n",
    "          df.to_csv(fr'earnings/{symbol}_EARNINGS_DATA.csv', mode='w', sep=',', header=df.columns)\n",
    "          print(\"[+] DONE {}\".format(symbol))\n",
    "          time.sleep(math.ceil(12))\n",
    "        except KeyError as ke:\n",
    "          print('KeyError: quarterly Earnings missing for ' + str(symbol))\n",
    "          f.write('KeyError: quarterly Earnings missing for ' + str(symbol))\n",
    "          pass\n",
    "        except Exception as e:\n",
    "          print(\"An exception occured: {}\".format(e))\n",
    "          f.write(str(symbol) + '\\t' + str(e) + '\\n')\n",
    "          pass\n",
    "\n",
    "\n",
    "def av_get_adjusted_ohlc(api_key, symbol):\n",
    "    \"\"\" return adjusted OHLC for symbol for the year as a dataframe\"\"\"\n",
    "    url = f\"{BASE_URL}/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={symbol}&datatype=json&outputsize=full&apikey={api_key}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return pd.DataFrame(response.json()['Time Series (Daily)'])\n",
    "\n",
    "def get_bulk_us_ohlc_data(filename, start_date='', end_date=''):\n",
    "    symbols = read_symbols(filename=filename)\n",
    "\n",
    "    for symbol in symbols:\n",
    "      df = av_get_adjusted_ohlc(api_key=AV_API_KEY, symbol=symbol)\n",
    "      df = df.T      \n",
    "      df['date'] = df.index\n",
    "      if start_date and end_date:\n",
    "        filtered_df = df.loc[(df['date'] > start_date) & (df['date'] < end_date)]\n",
    "      else:\n",
    "        filtered_df = df\n",
    "      # write to a CSV file\n",
    "      filtered_df.to_csv(rf\"ohlc/{symbol}_ADJUSTED_OHLC.txt\", header=filtered_df.columns, sep=',', mode='w')\n",
    "      time.sleep(12) # limit is 5 requests / minute\n",
    "      print(f\"[+] {symbol} DONE\")\n",
    "\n",
    "def get_us_ohlc_data(symbol, start_date='', end_date=''):\n",
    "    df = av_get_adjusted_ohlc(api_key=AV_API_KEY, symbol=symbol)\n",
    "    df = df.T      \n",
    "    df['date'] = df.index\n",
    "    if start_date and end_date:\n",
    "      filtered_df = df.loc[(df['date'] > start_date) & (df['date'] < end_date)]\n",
    "    else:\n",
    "      filtered_df = df\n",
    "    # write to a CSV file\n",
    "    filtered_df.to_csv(rf\"tmp/{symbol}_ADJUSTED_OHLC.txt\", header=filtered_df.columns, sep=',', mode='w')\n",
    "    print(f\"[+] {symbol} DONE\")\n",
    "\n",
    "def av_get_earnings(api_key, ticker):\n",
    "    ticker = ticker.strip().upper()\n",
    "\n",
    "    url = f\"{BASE_URL}/query?function=EARNINGS&symbol={ticker}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    raw_data = response.json()\n",
    "    if 'quarterlyEarnings' in raw_data:\n",
    "        return pd.DataFrame(raw_data['quarterlyEarnings'])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def av_get_income_statement(api_key, symbol):\n",
    "    url = rf\"{BASE_URL}/query?function=INCOME_STATEMENT&symbol={symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    frame = pd.DataFrame(response.json()['quarterlyReports'])\n",
    "    frame.to_csv(rf'income_stmts/{symbol}_INCOME_STATEMENT.csv', mode='w', sep=',', header=frame.columns)\n",
    "\n",
    "\n",
    "remaining =  list( set(read_symbols('stocks/FINAL_US_STOCKS.csv')) - set(parse_symbols('earnings/')))\n",
    "print(\"Remaining\", len(remaining))\n",
    "# get_bulk_earnings_data()\n",
    "# df = av_get_earnings(AV_API_KEY, \"META\")\n",
    "# print(df)\n",
    "# earnings_data = av_get_earnings(AV_API_KEY, symbol)\n",
    "# earnings_data.to_csv(f\"earnings/{symbol}_EARNINGS_DATA.txt\", sep=',', mode='w', header=earnings_data.columns)\n",
    "# get_bulk_us_ohlc_data(\"tmp/US_STOCKS.txt\")\n",
    "# get_us_ohlc_data(symbol=symbol)\n",
    "# get adjusted OHLC data for all US_stocks for the year 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "BASE_URL = r\"https://www.alphavantage.co\"\n",
    "AV_API_KEY = 'IHLNUQ0G66C0MJID'  # basil api key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_api(func, api_key, symbol):\n",
    "    assert symbol is not None\n",
    "    symbol = symbol.strip().upper()\n",
    "\n",
    "    url = f\"{BASE_URL}/query?function={func}&symbol={symbol}&apikey={api_key}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if func == 'EARNINGS':\n",
    "        return pd.DataFrame(response.json()['quarterlyEarnings'])\n",
    "    elif func == 'BALANCE_SHEET':\n",
    "        return pd.DataFrame(response.json()['quarterlyReports'])\n",
    "    elif func == 'INCOME_STATEMENT':\n",
    "        return pd.DataFrame(response.json()['quarterlyReports'])\n",
    "\n",
    "symbol = 'META'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_dir(dirname):\n",
    "    lst = os.listdir(dirname)\n",
    "    lst = [e.split('_')[0] for e in lst]\n",
    "    return lst\n",
    "\n",
    "symbols = read_dir('ohlc')\n",
    "\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        df3 = av_api('BALANCE_SHEET', AV_API_KEY, symbol)\n",
    "        df3.to_csv('sharesoutstanding/{}_SHARESOUTSTANDING.csv'.format(symbol), mode='w', sep=',', header=df3.columns)\n",
    "        print(\"[+] Done {} of {}\".format(symbols.index(symbol), len(symbols)))\n",
    "        time.sleep(0.1)\n",
    "    except Exception as e:\n",
    "        print(\"Exception\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yahooquery as yq\n",
    "t = yq.Ticker('aapl')\n",
    "t.summary_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "def parse_symbols(dirname):\n",
    "   files = os.listdir(dirname)\n",
    "   return [e[:e.index('_')] for e in files]\n",
    "\n",
    "def read_symbols(filename, remove_header=0):\n",
    "    with open(filename, \"r\") as file:\n",
    "      data = file.read().split('\\n')\n",
    "      data = [d.split(',')[1] for d in data if d != '']\n",
    "      if remove_header == 1:\n",
    "        data.pop(0) # ignore header\n",
    "    return data\n",
    "\n",
    "def get_done_symbols():\n",
    "    data = parse_symbols('data/')\n",
    "    data = pd.Series(data)\n",
    "    data.to_csv('done/symbols.csv', mode='w', sep=',', header=None)\n",
    "    return True\n",
    "\n",
    "def get_av_data(symbol):\n",
    "    #For reported Earnings & Analyst Estimated EPS, totalRevenue, commonStockSharesOutstanding\n",
    "    df1 = av_api('EARNINGS',AV_API_KEY, symbol)\n",
    "    df2 = av_api('INCOME_STATEMENT',AV_API_KEY, symbol)[['fiscalDateEnding','totalRevenue']]\n",
    "    df1 = df1.merge(df2, on='fiscalDateEnding', how='left')\n",
    "    df3 = av_api('BALANCE_SHEET',AV_API_KEY, symbol)[['fiscalDateEnding','commonStockSharesOutstanding']]\n",
    "    df1 = df1.merge(df3, on='fiscalDateEnding', how='left')\n",
    "\n",
    "    return df1\n",
    "\n",
    "def get_bulk_data():\n",
    "    symbols = read_symbols('stocks/FINAL_US_STOCKS.csv')\n",
    "    remaining =  list(set(symbols) - set(parse_symbols('data/')))\n",
    "    print(\"remaining\", len(remaining))\n",
    "    with open('logs/earnings_errors.txt', 'w') as f:\n",
    "        for symbol in remaining:\n",
    "            try:\n",
    "                df = get_av_data(symbol)\n",
    "                df.to_csv(fr'data/{symbol}_DATA.csv', mode='w', sep=',', header=df.columns)\n",
    "                print(\"[+] DONE {}: {} of {}\".format(symbol, remaining.index(symbol), len(remaining)))\n",
    "                time.sleep(1)\n",
    "            except KeyError as ke:\n",
    "                print('KeyError: quarterly Earnings missing for ' + str(symbol))\n",
    "                f.write('KeyError: quarterly Earnings missing for ' + str(symbol))\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(\"An exception occured: {}\".format(e))\n",
    "                f.write(str(symbol) + '\\t' + str(e) + '\\n')\n",
    "                pass\n",
    "\n",
    "\n",
    "def network_check():\n",
    "    while True:\n",
    "        def check_ping():\n",
    "            hostname = \"8.8.8.8\"\n",
    "            response = subprocess.call(['runas', '/user:Administrator', \"ping -c 1 \" + hostname])\n",
    "            print(response)\n",
    "            if response == 1:\n",
    "                pingstatus = 0\n",
    "            else: pingstatus = 1\n",
    "\n",
    "            return pingstatus\n",
    "        \n",
    "        def get_bulk_data():\n",
    "            symbols = read_symbols('stocks/FINAL_US_STOCKS.csv', remove_header=1)\n",
    "            done = read_symbols('done/symbols.csv')\n",
    "            remaining =  list(set(symbols) - set(done))\n",
    "            print(\"remaining\", len(remaining))\n",
    "            with open('logs/earnings_errors.txt', 'w') as f:\n",
    "                for symbol in remaining:\n",
    "                    try:\n",
    "                        df = get_av_data(symbol)\n",
    "                        df.to_csv(fr'data/{symbol}_DATA.csv', mode='w', sep=',', header=df.columns)\n",
    "                        print(\"[+] DONE {}: {} of {}\".format(symbol, remaining.index(symbol), len(remaining)))\n",
    "                        time.sleep(0.01)\n",
    "                    except KeyError as ke:\n",
    "                        print('KeyError: quarterly Earnings missing for ' + str(symbol))\n",
    "                        f.write('KeyError: quarterly Earnings missing for ' + str(symbol))\n",
    "                        pass\n",
    "                    except Exception as e:\n",
    "                        print(\"An exception occured: {}\".format(e))\n",
    "                        f.write(str(symbol) + '\\t' + str(e) + '\\n')\n",
    "                        pass\n",
    "        \n",
    "        net_stat = check_ping()\n",
    "\n",
    "        if net_stat == 0:\n",
    "            get_bulk_data()\n",
    "        else:\n",
    "            print(\"Resetting network adapters\")\n",
    "            dwnnw = \"ifconfig wlan0 down\"\n",
    "            upnw = \"ifconfig wlan0 up\"\n",
    "            os.system(dwnnw)\n",
    "            os.system(upnw)\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "\n",
    "# get_done_symbols()\n",
    "\n",
    "all = pd.read_csv('stocks/FINAL_US_STOCKS.csv')\n",
    "done = pd.read_csv('done/symbols.csv')\n",
    "\n",
    "remaining = list(set(all['Ticker']) - set(done['Ticker']))\n",
    "pd.Series(remaining).to_csv('done/remaining.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = read_symbols('stocks/FINAL_US_STOCKS.csv', remove_header=1)\n",
    "done = read_symbols('done/symbols.csv')\n",
    "remaining =  list(set(symbols) - set(done))\n",
    "print(len(remaining))\n",
    "print(remaining)\n",
    "remaining_df = pd.Series(remaining)\n",
    "remaining_df.to_csv('done/remaining.csv', mode='w', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get all OHLC data for 7k companies\n",
    "2. Get earnings data for each company in ohlc/\n",
    "3. Get company overview for each company in ohlc/\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# def get_bulk_us_earnings_data(filename):\n",
    "#     \"\"\"\n",
    "#     filename should be 'stocks/FINAL_US_STOCKS.csv\n",
    "#     \"\"\"\n",
    "#     symbols = read_symbols(filename=filename)\n",
    "\n",
    "#     for symbol in symbols:\n",
    "#         earnings_data = av_get_earnings(api_key=AV_API_KEY, ticker=symbol)\n",
    "#         earnings_data.to_csv(\n",
    "#             f\"earnings/{symbol}_EARNINGS_DATA.txt\", sep=',', mode='w', header=earnings_data.columns)\n",
    "#         time.sleep(12)\n",
    "\n",
    "def av_get_company_overview(api_key, symbol):\n",
    "    \"\"\"\n",
    "    deprecated\n",
    "    \"\"\"\n",
    "    symbol = symbol.strip().upper()\n",
    "\n",
    "    url = f\"{BASE_URL}/query?function=OVERVIEW&symbol={symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_bulk_us_overview(filename):\n",
    "    symbols = read_symbols(filename=filename)\n",
    "\n",
    "    for symbol in symbols:\n",
    "        overview = av_get_company_overview(api_key=AV_API_KEY, symbol=symbol)\n",
    "        with open(f\"overview/{symbol}_OVERVIEW.json\", \"w\") as f:\n",
    "            f.write(overview)\n",
    "        time.sleep(12)\n",
    "\n",
    "\n",
    "def build_gap_table(filename):\n",
    "    \"\"\"\n",
    "    deprecated\n",
    "    \"\"\"\n",
    "    symbols = read_symbols(filename=filename)\n",
    "\n",
    "    for symbol in symbols:\n",
    "        ohlc_df = pd.read_csv(f\"ohlc/{symbol}_ADJUSTED_OHLC.txt\")\n",
    "        earnings_dates = pd.read_csv(f\"earnings/{symbol}_EARNINGS_DATA.txt\")\n",
    "        ohlc_df = ohlc_df.set_index('date')\n",
    "        openings_at_ex_date_df = ohlc_df.loc[ohlc_df.index.isin(\n",
    "            earnings_dates)]\n",
    "        overview_json = pd.read_json(f\"overview/{symbol}_OVERVIEW.txt\")\n",
    "\n",
    "        myrows = []\n",
    "        # for each earnings date\n",
    "        for index, row in openings_at_ex_date_df.iterrows():\n",
    "            # find the closing date data before exdate\n",
    "            if ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0]:\n",
    "                c1 = ohlc_df.iloc[ohlc_df.index.get_loc(\n",
    "                    ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0])+1]\n",
    "            mydict = {}\n",
    "            mydict['exdate'] = index\n",
    "            mydict['Opening at ex date'] = row['1. open']\n",
    "            mydict['C1 date'] = c1['Unnamed: 0']\n",
    "            mydict['C1'] = c1['4. close']\n",
    "            mydict['Gap%'] = (row['1. open'] / c1['4. close'] - 1) * 100\n",
    "            mydict['Market Cap'] = overview_json['MarketCapitalization']\n",
    "            mydict['52W_dist'] = c1['4. close'] - float(overview_json['52WeekHigh'])\n",
    "\n",
    "            fiftytwowh_date_series = ohlc_df.loc[ohlc_df['2. high'] == float(\n",
    "                overview_json['52WeekHigh'])]['Unnamed: 0']\n",
    "\n",
    "            # TODO: Refactor\n",
    "            if fiftytwowh_date_series is not None:\n",
    "                c1_date = datetime.datetime.strptime(\n",
    "                    c1['Unnamed: 0'], \"%Y-%m-%d\").date()\n",
    "                fiftytwoWH_date = datetime.datetime.strptime(\n",
    "                    fiftytwowh_date_series[0], \"%Y-%m-%d\").date()\n",
    "\n",
    "                # doesn't make sense to calculate recency if NOT from the same year\n",
    "                if c1_date.year == fiftytwoWH_date.year:\n",
    "                    recency = c1_date - fiftytwoWH_date\n",
    "                else:  \n",
    "                    recency = ''\n",
    "            else:\n",
    "                recency = ''\n",
    "            \n",
    "            mydict['52W_recency'] = recency\n",
    "            mydict['Trading vol at exdate'] = row['6. volume']\n",
    "\n",
    "            myrows.append(mydict)\n",
    "        gap = pd.DataFrame(myrows)\n",
    "        gap.to_csv(f\"{symbol}_GAP_TABLE.txt\", header=gap.columns, sep=',', mode='w')\n",
    "        print(f\"[+] Constructed gap table for ${symbol}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"META\"\n",
    "earnings_dates = av_get_earnings(AV_API_KEY, symbol)['reportedDate']\n",
    "type(earnings_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_us_ohlc_data(\"META\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_df = pd.read_csv(f\"ohlc/{symbol}_ADJUSTED_OHLC.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_df = ohlc_df.set_index('date')\n",
    "openings_at_ex_date_df = ohlc_df.loc[ohlc_df.index.isin(earnings_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_get_company_overview(api_key, symbol):\n",
    "    symbol = symbol.strip().upper()\n",
    "    \n",
    "    url = f\"{BASE_URL}/query?function=OVERVIEW&symbol={symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_json = av_get_company_overview(AV_API_KEY, \"META\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = \"META\"\n",
    "def get_52_week_data(date):\n",
    "    \"\"\"\n",
    "    Return the 52 week (252 days) \n",
    "    Note: This function ignores the first 51 weeks of the historical OHLC as 52Wh is not defined\n",
    "    \"\"\"\n",
    "    myrows = []\n",
    "    for i in range(252):\n",
    "      try:\n",
    "        c1 = ohlc_df.iloc[ohlc_df.index.get_loc(ohlc_df.index[ohlc_df['Unnamed: 0'] == date].tolist()[0]) + i]\n",
    "        myrows.append(c1)\n",
    "      except IndexError:\n",
    "         pass\n",
    "    frame = pd.DataFrame(myrows)\n",
    "    high = max(frame['2. high'])\n",
    "    high_date = frame['2. high'].idxmax()\n",
    "    low_date = frame['3. low'].idxmin()\n",
    "    low = min(frame['3. low'])\n",
    "    wh52 = frame.index.get_indexer_for([high_date])\n",
    "    wl52 = frame.index.get_indexer_for([low_date])\n",
    "    return (high, int(wh52), low, int(wl52))\n",
    "\n",
    "def get_65_day_low(date):\n",
    "    myrows = []\n",
    "    ohlc_df = pd.read_csv(fr\"ohlc/{symbol}_ADJUSTED_OHLC.txt\")\n",
    "    for i in range(65):\n",
    "      try:\n",
    "        c1 = ohlc_df.iloc[ohlc_df.index.get_loc(ohlc_df.index[ohlc_df['Unnamed: 0'] == date].tolist()[0]) + i]\n",
    "        myrows.append(c1)\n",
    "      except IndexError:\n",
    "        pass\n",
    "    return min(pd.DataFrame(myrows)['4. close'])\n",
    "get_65_day_low('2023-02-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Ex date\n",
    "2. O\n",
    "3. H\n",
    "4. L\n",
    "5. C\n",
    "6. C1 date\n",
    "7. C1\n",
    "8. Gap%\n",
    "9. Mkt. Cap\n",
    "10. 52WH dist\n",
    "11. 52WH recency\n",
    "12. 52WL dist\n",
    "13. 52WL recency\n",
    "14. 65DL dist\n",
    "15. Day of the week\n",
    "16. Avg 20 days volume\n",
    "17. v0\n",
    "18. v1\n",
    "19. fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_volume(symbol=\"META\", start=\"\", days=20):\n",
    "    if start is None:\n",
    "      return None\n",
    "    ohlc_df = pd.read_csv(fr\"ohlc/{symbol}_ADJUSTED_OHLC.csv\")\n",
    "    myrows = []\n",
    "    for i in range(days):\n",
    "      try:\n",
    "        vols = ohlc_df.iloc[ohlc_df.index.get_loc(ohlc_df.index[ohlc_df['Unnamed: 0'] == start].tolist()[0]) + i]\n",
    "        myrows.append(vols['6. volume'])\n",
    "      except IndexError:\n",
    "         pass      \n",
    "    return pd.Series(myrows).mean()\n",
    "\n",
    "get_avg_volume(\"META\", \"2023-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earnings_data = pd.read_csv(fr\"earnings/{symbol}_EARNINGS_DATA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_get_income_statement(api_key, symbol):\n",
    "    url = rf\"{BASE_URL}/query?function=INCOME_STATEMENT&symbol={symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    frame = pd.DataFrame(response.json()['quarterlyReports'])\n",
    "    frame.to_csv(rf'income_stmts/{symbol}_INCOME_STATEMENT.csv', mode='w', sep=',', header=frame.columns)\n",
    "\n",
    "def get_fundamental_data(symbol, exdate):\n",
    "    earnings = pd.read_csv(fr\"earnings/{symbol}_EARNINGS_DATA.txt\")\n",
    "    income_stmts = pd.read_csv(fr\"income_stmts/{symbol}_INCOME_STATEMENT.csv\")\n",
    "    row = earnings[earnings['reportedDate'] == exdate]\n",
    "\n",
    "    reported_eps = row[['reportedEPS', 'estimatedEPS', 'surprise']]\n",
    "    # return type(float(reported_eps))\n",
    "    return reported_eps\n",
    "    # return (reported_eps, estimated_eps, surprise)\n",
    "    # return (float(reported_eps), float(estimated_eps), float(surprise))\n",
    "get_fundamental_data('META','2023-02-01')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import calendar\n",
    "myrows = []\n",
    "for index, row in openings_at_ex_date_df.iterrows():\n",
    "    if ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0]:\n",
    "        c1 = ohlc_df.iloc[ohlc_df.index.get_loc(\n",
    "            ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0])+1]\n",
    "    mydict = {}\n",
    "    mydict['exdate'] = index\n",
    "    mydict['weekday'] = calendar.day_name[datetime.datetime.strptime(index, \"%Y-%m-%d\").date().weekday()]\n",
    "    mydict['O'] = row['1. open']\n",
    "    mydict['H'] = row['2. high']\n",
    "    mydict['L'] = row['3. low']\n",
    "    mydict['C'] = row['4. close']\n",
    "    mydict['C1 date'] = c1['Unnamed: 0']\n",
    "    mydict['C1'] = c1['4. close']\n",
    "    mydict['Gap%'] = (row['1. open'] / c1['4. close'] - 1) * 100\n",
    "\n",
    "    # TODO: Market cap\n",
    "    # 1. Market cap = Total shares outstanding * price at that date\n",
    "    mydict['MktCap'] = float(overview_json['SharesOutstanding']) * float(row['1. open'])\n",
    "\n",
    "    # TODO: Compute 52WH from OHLC data (52*7)\n",
    "    # Formula: (52WH - Close) / 52WH\n",
    "    WH_52, WH52_recency, WL_52, WL52_recency = get_52_week_data(c1['Unnamed: 0'])\n",
    "    DL_65 = get_65_day_low(c1['Unnamed: 0'])\n",
    "    # print(WH_52, WH52_recency, WL_52, WL52_recency, \"C1\", c1['Unnamed: 0'])\n",
    "\n",
    "    mydict['52WH_dist'] = (WH_52 - c1['4. close']) / WH_52\n",
    "    mydict['52WH_recency'] = WH52_recency\n",
    "    mydict['52WL_dist'] = (c1['4. close'] - WH_52) / WL_52\n",
    "    mydict['52WL_recency'] = WL52_recency\n",
    "\n",
    "    # TODO: check validity\n",
    "    mydict['65DL_dist'] = (c1['4. close'] - DL_65) / DL_65\n",
    "\n",
    "    # TODO: 52WH_recency\n",
    "    # Redo with respect to new defintion\n",
    "    # fiftytwowh_date_series = ohlc_df.loc[ohlc_df['2. high'] == float(\n",
    "    #     overview_json['52WeekHigh'])]['Unnamed: 0']\n",
    "    # if fiftytwowh_date_series is not None:\n",
    "    #     c1_date = datetime.datetime.strptime(\n",
    "    #         c1['Unnamed: 0'], \"%Y-%m-%d\").date()\n",
    "    #     fiftytwoWH_date = datetime.datetime.strptime(\n",
    "    #         fiftytwowh_date_series[0], \"%Y-%m-%d\").date()\n",
    "    #     if c1_date.year == fiftytwoWH_date.year:\n",
    "    #         recency = c1_date - fiftytwoWH_date\n",
    "    #     else:\n",
    "    #         recency = ''\n",
    "    # else:\n",
    "    #     recency = ''\n",
    "\n",
    "\n",
    "    mydict['v0'] = row['6. volume']\n",
    "    mydict['v1'] = c1['6. volume']\n",
    "    mydict['avg_v20'] = get_avg_volume(symbol=\"META\", start=c1['Unnamed: 0'], days=20)\n",
    "    # TODO: Get vol1 as well\n",
    "\n",
    "    # FUNDAMENTALS\n",
    "    fundamental_data = get_fundamental_data(symbol='META', exdate=row['Unnamed: 0'])\n",
    "    # print(type(fundamental_data['estimatedEPS']))\n",
    "    mydict['reportedEPS'] = fundamental_data['reportedEPS']\n",
    "    mydict['estimatedEPS'] = fundamental_data['estimatedEPS']\n",
    "    mydict['surprise'] = fundamental_data['surprise']\n",
    "\n",
    "    myrows.append(mydict)\n",
    "\n",
    "gap = pd.DataFrame(myrows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap.to_csv(\"tmp/META_GAP_TABLE.csv\", mode='w', sep=',', header=gap.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap = pd.read_csv(rf\"tmp/{symbol}_GAP_TABLE.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
