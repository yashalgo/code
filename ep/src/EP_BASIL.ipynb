{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption: The earnings date is taken as the ex-date\n",
    "\n",
    "1. Get list of company tickers from finviz\n",
    "2. Get earnings data report for each company in `companies`\n",
    "3. Make a DF row for each company row\n",
    "\n",
    "Columns to populate:\n",
    "For each company: For each earnings\n",
    "1. compute gap%\n",
    "2. find C1\n",
    "3. find O\n",
    "4. find market cap\n",
    "5. find distance from 52W high\n",
    "6. find recency of 52W high\n",
    "7. trading volume near announcement (at exdate)\n",
    "8. trading volume before announcement (at c1)\n",
    "\n",
    "FORMULAE:\n",
    "1. Market cap = Total shares outstanding * price at that date\n",
    "2. 52W_HIGH = \n",
    "3. 52W_RECENCY = \n",
    "4. \n",
    "\n",
    "\n",
    "CHANGES:\n",
    "1. Change 52week defintion to 252 days instead of 364 days.\n",
    "2. for the first week, do min (52 weeks, available weeks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Completed Taske [##############################] 1/1 \n"
     ]
    }
   ],
   "source": [
    "# import finviz\n",
    "from finvizfinance.screener.custom import Custom\n",
    "\n",
    "def get_us_stocks():\n",
    "    fcustom = Custom()\n",
    "    cols = [0, 1]\n",
    "    filters = { 'Country': 'USA' }\n",
    "    fcustom.set_filter(filters_dict=filters)\n",
    "    df1 = fcustom.screener_view(columns=cols, select_page=1, sleep_sec=1, order='Ticker')\n",
    "    return df1\n",
    "\n",
    "def write_to_file(df, filename):\n",
    "    df.to_csv(rf\"{filename}\", header=None, sep=' ', mode='w')\n",
    "\n",
    "write_to_file(get_us_stocks(), \"stocks/US_STOCKS.txt\")\n",
    "print(\"[+] Completed Task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Time Series (Daily)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 58\u001b[0m\n\u001b[0;32m     54\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(response\u001b[39m.\u001b[39mjson()[\u001b[39m'\u001b[39m\u001b[39mquarterlyEarnings\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 58\u001b[0m get_bulk_us_ohlc_data(\u001b[39m\"\u001b[39;49m\u001b[39mtmp/US_STOCKS.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m \u001b[39m# get adjusted OHLC data for all US_stocks for the year 2022\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 26\u001b[0m, in \u001b[0;36mget_bulk_us_ohlc_data\u001b[1;34m(filename, start_date, end_date)\u001b[0m\n\u001b[0;32m     23\u001b[0m symbols \u001b[39m=\u001b[39m read_symbols(filename\u001b[39m=\u001b[39mfilename)\n\u001b[0;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m symbol \u001b[39min\u001b[39;00m symbols:\n\u001b[1;32m---> 26\u001b[0m   df \u001b[39m=\u001b[39m av_get_adjusted_ohlc(api_key\u001b[39m=\u001b[39;49mAV_API_KEY, symbol\u001b[39m=\u001b[39;49msymbol)\n\u001b[0;32m     27\u001b[0m   df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mT      \n\u001b[0;32m     28\u001b[0m   df[\u001b[39m'\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mindex\n",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m, in \u001b[0;36mav_get_adjusted_ohlc\u001b[1;34m(api_key, symbol)\u001b[0m\n\u001b[0;32m     16\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mBASE_URL\u001b[39m}\u001b[39;00m\u001b[39m/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol=\u001b[39m\u001b[39m{\u001b[39;00msymbol\u001b[39m}\u001b[39;00m\u001b[39m&datatype=json&outputsize=full&apikey=\u001b[39m\u001b[39m{\u001b[39;00mapi_key\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m---> 20\u001b[0m \u001b[39mreturn\u001b[39;00m pd\u001b[39m.\u001b[39mDataFrame(response\u001b[39m.\u001b[39;49mjson()[\u001b[39m'\u001b[39;49m\u001b[39mTime Series (Daily)\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Time Series (Daily)'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "BASE_URL = 'https://alphavantage.co'\n",
    "AV_API_KEY = 'IHLNUQ0G66C0MJID'\n",
    "\n",
    "def read_symbols(filename):\n",
    "    with open(filename, \"r\") as file:\n",
    "      data = file.read().split('\\n')\n",
    "      data = [d.split(' ')[1] for d in data]\n",
    "    return data\n",
    "\n",
    "def av_get_adjusted_ohlc(api_key, symbol):\n",
    "    \"\"\" return adjusted OHLC for symbol for the year as a dataframe\"\"\"\n",
    "    url = f\"{BASE_URL}/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={symbol}&datatype=json&outputsize=full&apikey={api_key}\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return pd.DataFrame(response.json()['Time Series (Daily)'])\n",
    "\n",
    "def get_bulk_us_ohlc_data(filename, start_date='', end_date=''):\n",
    "    symbols = read_symbols(filename=filename)\n",
    "\n",
    "    for symbol in symbols:\n",
    "      df = av_get_adjusted_ohlc(api_key=AV_API_KEY, symbol=symbol)\n",
    "      df = df.T      \n",
    "      df['date'] = df.index\n",
    "      if start_date and end_date:\n",
    "        filtered_df = df.loc[(df['date'] > start_date) & (df['date'] < end_date)]\n",
    "      else:\n",
    "        filtered_df = df\n",
    "      # write to a CSV file\n",
    "      filtered_df.to_csv(rf\"ohlc/{symbol}_ADJUSTED_OHLC.txt\", header=filtered_df.columns, sep=',', mode='w')\n",
    "      time.sleep(12) # limit is 5 requests / minute\n",
    "      print(f\"[+] {symbol} DONE\")\n",
    "\n",
    "def get_us_ohlc_data(symbol, start_date='', end_date=''):\n",
    "    df = av_get_adjusted_ohlc(api_key=AV_API_KEY, symbol=symbol)\n",
    "    df = df.T      \n",
    "    df['date'] = df.index\n",
    "    if start_date and end_date:\n",
    "      filtered_df = df.loc[(df['date'] > start_date) & (df['date'] < end_date)]\n",
    "    else:\n",
    "      filtered_df = df\n",
    "    # write to a CSV file\n",
    "    filtered_df.to_csv(rf\"ohlc/{symbol}_ADJUSTED_OHLC.txt\", header=filtered_df.columns, sep=',', mode='w')\n",
    "    print(f\"[+] {symbol} DONE\")\n",
    "\n",
    "def av_get_earnings(api_key, ticker):\n",
    "    ticker = ticker.strip().upper()\n",
    "\n",
    "    url = f\"{BASE_URL}/query?function=EARNINGS&symbol={ticker}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return pd.DataFrame(response.json()['quarterlyEarnings'])\n",
    "\n",
    "get_bulk_us_ohlc_data(\"tmp/US_STOCKS.txt\")\n",
    "# get adjusted OHLC data for all US_stocks for the year 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def get_bulk_us_earnings_date(filename):\n",
    "    symbols = read_symbols(filename=filename)\n",
    "\n",
    "    for symbol in symbols:\n",
    "        earnings_date = av_get_earnings(api_key=AV_API_KEY, ticker=symbol)\n",
    "        earnings_date.to_csv(\n",
    "            f\"earnings/{symbol}_EARNINGS_DATE.txt\", sep=',', mode='w', header=earnings_date.columns)\n",
    "        time.sleep(12)\n",
    "\n",
    "\n",
    "def av_get_company_overview(api_key, symbol):\n",
    "    symbol = symbol.strip().upper()\n",
    "\n",
    "    url = f\"{BASE_URL}/query?function=OVERVIEW&symbol={symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_bulk_us_overview(filename):\n",
    "    symbols = read_symbols(filename=filename)\n",
    "\n",
    "    for symbol in symbols:\n",
    "        overview = av_get_company_overview(api_key=AV_API_KEY, symbol=symbol)\n",
    "        with open(f\"overview/{symbol}_OVERVIEW.json\", \"w\") as f:\n",
    "            f.write(overview)\n",
    "        time.sleep(12)\n",
    "\n",
    "\n",
    "def build_gap_table(filename):\n",
    "    symbols = read_symbols(filename=filename)\n",
    "\n",
    "    for symbol in symbols:\n",
    "        ohlc_df = pd.read_csv(f\"data/{symbol}_ADJUSTED_OHLC.txt\")\n",
    "        earnings_dates = pd.read_csv(f\"earnings/{symbol}_EARNINGS_DATE.txt\")\n",
    "        ohlc_df = ohlc_df.set_index('date')\n",
    "        openings_at_ex_date_df = ohlc_df.loc[ohlc_df.index.isin(\n",
    "            earnings_dates)]\n",
    "        overview_json = pd.read_json(f\"overview/{symbol}_OVERVIEW.txt\")\n",
    "\n",
    "        myrows = []\n",
    "        # for each earnings date\n",
    "        for index, row in openings_at_ex_date_df.iterrows():\n",
    "            # find the closing date data before exdate\n",
    "            if ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0]:\n",
    "                c1 = ohlc_df.iloc[ohlc_df.index.get_loc(\n",
    "                    ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0])+1]\n",
    "            mydict = {}\n",
    "            mydict['exdate'] = index\n",
    "            mydict['Opening at ex date'] = row['1. open']\n",
    "            mydict['C1 date'] = c1['Unnamed: 0']\n",
    "            mydict['C1'] = c1['4. close']\n",
    "            mydict['Gap%'] = (row['1. open'] / c1['4. close'] - 1) * 100\n",
    "            mydict['Market Cap'] = overview_json['MarketCapitalization']\n",
    "            mydict['52W_dist'] = c1['4. close'] - float(overview_json['52WeekHigh'])\n",
    "\n",
    "            fiftytwowh_date_series = ohlc_df.loc[ohlc_df['2. high'] == float(\n",
    "                overview_json['52WeekHigh'])]['Unnamed: 0']\n",
    "\n",
    "            # TODO: Refactor\n",
    "            if fiftytwowh_date_series is not None:\n",
    "                c1_date = datetime.datetime.strptime(\n",
    "                    c1['Unnamed: 0'], \"%Y-%m-%d\").date()\n",
    "                fiftytwoWH_date = datetime.datetime.strptime(\n",
    "                    fiftytwowh_date_series[0], \"%Y-%m-%d\").date()\n",
    "\n",
    "                # doesn't make sense to calculate recency if NOT from the same year\n",
    "                if c1_date.year == fiftytwoWH_date.year:\n",
    "                    recency = c1_date - fiftytwoWH_date\n",
    "                else:  \n",
    "                    recency = ''\n",
    "            else:\n",
    "                recency = ''\n",
    "            \n",
    "            mydict['52W_recency'] = recency\n",
    "            mydict['Trading vol at exdate'] = row['6. volume']\n",
    "\n",
    "            myrows.append(mydict)\n",
    "        gap = pd.DataFrame(myrows)\n",
    "        gap.to_csv(f\"{symbol}_GAP_TABLE.txt\", header=gap.columns, sep=',', mode='w')\n",
    "        print(f\"[+] Constructed gap table for ${symbol}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol = \"META\"\n",
    "earnings_dates = av_get_earnings(AV_API_KEY, symbol)['reportedDate']\n",
    "type(earnings_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_df = pd.read_csv(f\"data/{symbol}_ADJUSTED_OHLC.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_df = ohlc_df.set_index('date')\n",
    "openings_at_ex_date_df = ohlc_df.loc[ohlc_df.index.isin(earnings_dates)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def av_get_company_overview(api_key, symbol):\n",
    "    symbol = symbol.strip().upper()\n",
    "    \n",
    "    url = f\"{BASE_URL}/query?function=OVERVIEW&symbol={symbol}&apikey={api_key}\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 15\n"
     ]
    }
   ],
   "source": [
    "overview_json = av_get_company_overview(AV_API_KEY, \"META\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 9)\n",
      "(384.33, 355, 88.09, 58)\n"
     ]
    }
   ],
   "source": [
    "symbol = \"META\"\n",
    "def get_52_week_data(date):\n",
    "    \"\"\"\n",
    "    Return the 52 week (52*7 = 364 days) high \n",
    "    Note: This function ignores the first 51 weeks of the historical OHLC as 52Wh is not defined\n",
    "    \"\"\"\n",
    "    myrows = []\n",
    "    for i in range(52*7):\n",
    "      try:\n",
    "        c1 = ohlc_df.iloc[ohlc_df.index.get_loc(ohlc_df.index[ohlc_df['Unnamed: 0'] == date].tolist()[0]) + i]\n",
    "        myrows.append(c1)\n",
    "      except IndexError:\n",
    "         pass\n",
    "    frame = pd.DataFrame(myrows)\n",
    "    frame.to_csv(\"tmp/DATA.csv\", mode='w', header=frame.columns)\n",
    "    high = max(frame['2. high'])\n",
    "    high_date = frame['2. high'].idxmax()\n",
    "    low_date = frame['3. low'].idxmin()\n",
    "    low = min(frame['3. low'])\n",
    "    print(frame.shape)\n",
    "    wh52 = frame.index.get_indexer_for([high_date])\n",
    "    wl52 = frame.index.get_indexer_for([low_date])\n",
    "    return (high, int(wh52), low, int(wl52))\n",
    "\n",
    "print(get_52_week_data('2023-01-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openings_at_ex_date_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdatetime\u001b[39;00m\n\u001b[0;32m      2\u001b[0m myrows \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m openings_at_ex_date_df\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m ohlc_df\u001b[39m.\u001b[39mindex[ohlc_df[\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m]:\n\u001b[0;32m      5\u001b[0m         c1 \u001b[39m=\u001b[39m ohlc_df\u001b[39m.\u001b[39miloc[ohlc_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mget_loc(\n\u001b[0;32m      6\u001b[0m             ohlc_df\u001b[39m.\u001b[39mindex[ohlc_df[\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mUnnamed: 0\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mtolist()[\u001b[39m0\u001b[39m])\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openings_at_ex_date_df' is not defined"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "myrows = []\n",
    "for index, row in openings_at_ex_date_df.iterrows():\n",
    "    if ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0]:\n",
    "        c1 = ohlc_df.iloc[ohlc_df.index.get_loc(\n",
    "            ohlc_df.index[ohlc_df['Unnamed: 0'] == row['Unnamed: 0']].tolist()[0])+1]\n",
    "    mydict = {}\n",
    "    mydict['exdate'] = index\n",
    "    mydict['O'] = row['1. open']\n",
    "    mydict['C1 date'] = c1['Unnamed: 0']\n",
    "    mydict['C1'] = c1['4. close']\n",
    "    mydict['Gap%'] = (row['1. open'] / c1['4. close'] - 1) * 100\n",
    "\n",
    "\n",
    "    # TODO: Market cap\n",
    "    # 1. Market cap = Total shares outstanding * price at that date\n",
    "    mydict['MktCap'] = float(overview_json['SharesOutstanding']) * float(row['1. open'])\n",
    "\n",
    "    # TODO: Compute 52WH from OHLC data (52*7)\n",
    "    # Formula: (52WH - Close) / 52WH\n",
    "    WH_52, WH52_recency, WL_52, WL52_recency = get_52_week_data(c1['Unnamed: 0'])\n",
    "    # print(WH_52, WH52_recency, WL_52, WL52_recency, \"C1\", c1['Unnamed: 0'])\n",
    "\n",
    "    mydict['52WH_dist'] = (WH_52 - c1['4. close']) / WH_52\n",
    "    mydict['52WL_dist'] = (c1['4. close'] - WH_52) / WL_52\n",
    "\n",
    "    # TODO: 52WH_recency\n",
    "    # Redo with respect to new defintion\n",
    "    # fiftytwowh_date_series = ohlc_df.loc[ohlc_df['2. high'] == float(\n",
    "    #     overview_json['52WeekHigh'])]['Unnamed: 0']\n",
    "    # if fiftytwowh_date_series is not None:\n",
    "    #     c1_date = datetime.datetime.strptime(\n",
    "    #         c1['Unnamed: 0'], \"%Y-%m-%d\").date()\n",
    "    #     fiftytwoWH_date = datetime.datetime.strptime(\n",
    "    #         fiftytwowh_date_series[0], \"%Y-%m-%d\").date()\n",
    "    #     if c1_date.year == fiftytwoWH_date.year:\n",
    "    #         recency = c1_date - fiftytwoWH_date\n",
    "    #     else:\n",
    "    #         recency = ''\n",
    "    # else:\n",
    "    #     recency = ''\n",
    "\n",
    "    # Formula: C1 date - 52WH date\n",
    "    # mydict['52WH_recency'] = (datetime.datetime.strptime(c1['Unnamed: 0'], \"%Y-%m-%d\").date() - datetime.datetime.strptime(WH_52_date, \"%Y-%m-%d\").date()).days // 7\n",
    "    # mydict['52WL_recency'] = (datetime.datetime.strptime(c1['Unnamed: 0'], \"%Y-%m-%d\").date() - datetime.datetime.strptime(WL_52_date, \"%Y-%m-%d\").date()).days// 7\n",
    "    mydict['52WH_recency'] = WH52_recency\n",
    "    mydict['52WL_recency'] = WL52_recency\n",
    "\n",
    "    mydict['v0'] = row['6. volume']\n",
    "    mydict['v1'] = c1['6. volume']\n",
    "    # TODO: Get vol1 as well\n",
    "\n",
    "    myrows.append(mydict)\n",
    "\n",
    "gap = pd.DataFrame(myrows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap.to_csv(\"tmp/META_GAP_TABLE.csv\", mode='w', sep=',', header=gap.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap.to_csv(f\"META_GAP_TABLE.csv\", mode=\"w\", sep=\",\", header=gap.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a9dfcd88bfd6f99f09e1f4fb417a5dfc2105cfcf3751885989f66dd3e94985d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
